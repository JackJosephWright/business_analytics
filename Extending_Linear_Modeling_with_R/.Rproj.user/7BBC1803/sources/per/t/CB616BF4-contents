---
title: "Binomial_Data"
author: "Jack Wright"
date: "10/25/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


## Challenger Disaster Example



Rubber O-ring example

```{r}
library(faraway)
data(orings)
plot (damage/6 ~ temp, orings, xlim=c(25,85), ylim =
c(0,1),
 xlab="Temperature", ylab="Prob of damage")
```
```{r}
library(tidyverse)
ggplot(orings, aes(y=damage/6,x=temp))+geom_point()+xlab('Temperature')+ylab('Prob of Damage')
```

interested in how the probability of failure in a given O-ring is related to the launch temp and predicting the probability when the temp is 31F.


NAIVE APPROACH
```{r}
lmod<-lm(damage/6~temp, orings)
library(ggResidpanel)
ggplot(orings, aes(x=temp,y=damage/6))+geom_point()+geom_smooth(method='lm', se=FALSE)
```

```{r}
summary(lmod)
```
FOR LINEAR MODEL:

-require errors to be appro normal (theyre not)

NOTE:

variance of a binomial variable is not constant( recall variance is based on the mean and is meaningless)


NO LINEAR MODEL


## Binomial Regression Model

suppose response variable (Y_i) for i =1 ...n_i is binomially distributed

(probabilities of k successes on n trials , looks normal around mean? )


assume each trial is INDEPENDENT and subject to the same predictors.

The group of trials is called a *covariate class*

need a model that describes relationship of predictors(x_1..x_i) to p


we can express the effect of the predictors on the response solely through the *linear predictor* (the model) allows us to extend to models for other types of response and is part of the class *generalized linear models* or GLMs. 


we use a link function to get the probability


Three common choices:
(where p = probability of success)
1.

Logit:

$$\eta=log(p/(1-p)$$



2.

Probit

$$\eta=\Phi^{-1}(p)$$

where phi^-1 is the inverse normal cumulative distribution function

(think of a cumulative plot of a normal distribution)

3.

Complimentary log-log:

$$\eta=log(-log(1-p))$$


IDEA OF LINK FUNCTION:

central idea of generalized linear models


links the linear predictor to the mean of the response in the wider class of models


CHOOSE THE LINK FUNCTION LATER:


FIRST CALCULATE LOG LIKELIHOOD:

```{r}
logitmod<-glm(cbind(damage,6-damage)~temp, family=binomial(), data=orings)
summary(logitmod)
```
*for me checking the pchsiq
```{r}
pchisq(16.912, 21, lower=FALSE)
```

*i think, becuase the alpha is high, we reject the need for the saturated model*

```{r}
library(car)
mmps(logitmod)
```

*marginal model plots do not look good. 

**BACK TO BOOK**

for binomial response we need 2 sets of info about the response

y:
 successes
n:
  number of trials
  
OR
n-y:
  number of failures
  

```{r}
plot (damage/6 ~ temp, orings, xlim=c(25,85),
ylim=c(0,1),
 xlab="Temperature", ylab="Prob of damage")
x <- seq(25,85,1)
lines(x,ilogit(11.6630−0.2162*x))

```

Notice how the log fit tends towards zero, starts out at 1. 


*compare with probit fit*

```{r}
probitmod<-glm(cbind(damage,6-damage)~temp, family=binomial(link=probit),orings)
summary(probitmod)
```

```{r}
plot (damage/6 ~ temp, orings, xlim=c(25,85),
ylim=c(0,1),
 xlab="Temperature", ylab="Prob of damage")
x <- seq(25,85,1)
lines(x,ilogit(11.6630−0.2162*x))
lines(x, pnorm(5.5915-0.1058*x), lty=2) 

```

dotted line is probit, solid is logit

```{r}
l<-ilogit (11.6630-0.2162*31)
p<-pnorm(5.5915-0.1058*31) 

cat('logit estimation at 31F:',l,'\nProbit estimation at 31F:',p)
```
both have a very high probability of damage 



## Inference


consider 2 models

larger model with l params and L_l likelihood

smaller model with s paramaters and L_s likelihoood
  linear subspace(linear restriction on params of       larger model)
  
suggests likelihood ratio statistic

$$2log\frac{L_L}{L_S}$$

good for comparing models


now choose saturated larger model (SATURATED MODEL)


since the saturated model has perfect fit, the deviance D (or G^2) measures how close the smaller model comes to perfection.


*Deviance is a measure of goodness of fit*


residual deviance:

deviance for current model

null deviance:

deviance for model with no preditors, just an intercept term

If the model is sufficiently large, the deviance is a chisq distribution with n-l (the df for the two models) degrees of freedom IF THE MODEL IS CORRECT

USE DEVIANCE to test if the model is an adequate fit. 


FOR LOGIT OF CHALLENGER EXAMPLE:

```{r}
pchisq(deviance(logitmod), df.residual(logitmod),lower=FALSE)
```
recall:

H_0: our model (less complex)

H_A: saturated model (more complex)

since the P-value is HIGH, we say that it fits the data well

*THIS DOES NOT MEAN THAT YOU CANT FIND A BETTER MODEL*

check the null model to see the p-value

```{r}
pchisq(38.9,22,lower=FALSE)

```

we see the fit is inadequate, the saturated is perferred over the null model


HELPFUL TIP:

if the deviance is much bigger than the DF, could reject the null hypotheesis without calculating p


when n is low (or m from other book)(number of samples per x) then the chisq doesnt approximate.


USE Hosmer-Lemeshow

*Rule of thumb, dont use this method for n<5


when n is too low use difference in deviance between the two models

BACK TO EXAMPLE:

since difference between df between the null model and the temp model is only 1, do this

```{r}
dev_dif<-(38.9-16.9)
pchisq(dev_dif,1,lower=FALSE)
```
since p is so small, we can conclude launch temp is statistically significant

IN THIS CASE:

H_0: null

H_A: our model


FINDING THE CONFIDENCE INTERVAL

```{r}
library(MASS)
confint(logitmod)
```


## Tolerance Distribution


student answers questions on a test and student has aptitude T. 

question might have difficulty d_i

only get answer correct if T>d_i

if d_i is fixed and $T\sim(\mu,\sigma2)$




## Interpreting Odds

odds are sometimes better scale than probability to represent chance.

arose as a way to express payoff for bets


Mathematical advantage of odds is that they are UNBOUNDED


odds form basis for subjective asssessment of probability


sometimes probabilities are determined from considerations of symmetry or long-term frequencies. (this might be unavailable)


example:

covariates x_1, x_2

logistic regression model

$$log(odds)=log(\frac{p}{1-p})=\beta_0+\beta_1x_1+\beta_2x_2$$


INTERPRETATION:

$\beta_1$:

unit increase in x_1 with x_2 held fixed, increases the log-odds of succes by \beta_1. OR 

increases odds of success by a factor of $e^{\beta_1}$

ALT IDEA for odds-ratio:

relative risk. 

p-succcess in some condition is p1 and in its absence p2.

RELATIVE RISK is $P_1/P_2$


for rare outcomes relative risk and odds ratio are similiar, but for larger probabilities there are differences. 



consider data from a study on infant respiratory disease.

```{r}
library(kableExtra)
data("babyfood")
xtabs(disease/(disease+nondisease)~sex+food, babyfood)%>%kbl()%>%kable_classic()
```

fit and examine the model
```{r}
mdl<-glm(cbind(disease,nondisease)~sex+food, family = binomial(), babyfood)
summary(mdl)
```


chisq can be expected to be accurate here due to large covariate class sizes (all greated than 5)


Is there a sex-by-food interaction? 


WHAT DOES THIS MEAN:

*notice that a model with the interation effect would be saturated with deviance and degrees of freedom zero, so we can look at the residual deviance of this model to test for the interaction effect*


.7 deviance is not large for 2 df, so we can conclude that there is no evidence of an interaction effect. 

MEANS we can interpret the main effects separately.


TEST SIGNIFICANCE OF MAIN EFFECTS:
```{r}
drop1(mdl,test="Chi")
```

`drop1` tests each predictor relative to the full model.

both predictors are significant. 


Consider interpretation of coefficients starting with breast feeding (from full model)

```{r}
exp(mdl$coefficients[3])
```

INTERPRETATION:

breast feeding reduces the odds of respiratory disease to 51% of that for BOTTLE FEEDING


GET CONFIDENCE INTERVAL:

(this is log scale)
```{r}
confint(mdl)
```

(take out of log scale)
```{r}
exp(confint(mdl))%>%kbl%>%kable_classic()
```



NOTE:

for small values on the log scale, they are about approximate to their actual probabilities

EX from model log-odds for suppliment

```{r}
#log odds for SUPPLIMENT to have  lower disease than bottle feeding
log_odds<-(-.173)
 1-exp(-0.17)
```
see that the real odds are about 16 and the log odds are about 17.

we see here that breast-fed and to a lesser extent suppliment-fed babies are less vulnerable to respiratory disease. 

ALSO see that boys are more vulnerable than girls


## Prospective and Retrospective Sampling


prospective sampling:

-predictors are fixed then outcome is observed


EX:

in infant respiratory example

select sample of newborn girls and boys whose parents chose a particular method of feeding, then monitor for their first year


this is a *cohort study*

retrospective sampling:

outcome is fixed then predictors are observed


EX:

find infants coming to a doctor with a respiratory disease in the first year then record their sex and method of feeding


ALSO:

obtain sample of healty infants and record the info. 


*case-control study*

HOW DO PREDICTORS AFFECT THE RESPONSE.

lets look at data
```{r}
a<-babyfood[c(1,3),]
a<-a%>%mutate(log_odds=log(disease/nondisease),.before=sex)
a%>%kbl%>%kable_classic()
```


the difference in the log odds$\Delta=-1.6-2.55=.65$ represents the increased risk of respiratory disease incurred by bottle feeding relative to breast feeding




Suppose this was a retrospective study. 

compute log-odds of feeding type given respiratory disease status then find the difference. 

log odds are the same

This shows that restrospective design is as effective as a prospective design for estimating $\Delta$



## Choice of Link Function


cant make based on data alone. 

for regions of moderate probability (in the middle, not close to zero or 1) they are all very similar


Larger differences in the tails, but you need a lot of data which makes it expensive to distinguish between the link functions in this region. 


make with assumptions derived from physical knowledge or convenience. 


*advantages and disadvantages of link funcions*

DATA: insects dying at different levels of insecticide concentration. 

Fit all 3 link functions
```{r}
data(bliss)
bliss

mod1<-glm(cbind(dead,alive)~conc,family=binomial(),data=bliss)
modp <- glm(cbind(dead, alive) ~ conc,
family=binomial(link=probit),
 data=bliss) 
modc<-glm(cbind(dead, alive) ~ conc,
family=binomial(link=cloglog),
 data=bliss) 
```


Start by looking at fitted values

```{r}
fitted(mod1)
```

You can also get these predicted values from the `predict` function

```{r}
predict (mod1, type='response')
```

You can also get them from `model$linear.predictors`


```{r}
#take the logit, because the predictors are from the logit function (these are the betas in the exponents.)


ilogit(mod1$linear.predictors)
```

notice the need to distinguish between predictions in the scale of the and the link (don't know what this means)

compare fitted values from all 3 models
```{r}

d<-data.frame('logit'=fitted(mod1),'probit'=fitted(modp),'loglog'=fitted(modc))
d%>%kbl%>%kable_classic_2()
```


these are not very different, but lets look at a wider range

```{r}
x<-seq(-2,8,0.2)
p1<-ilogit(mod1$coefficients[1]+mod1$coefficients[2]*x)
pp<-pnorm(modp$coefficients[1]+modp$coefficients[2]*x)
pc<-1-exp(-exp((modc$coefficients[1]+modc$coefficients[2]*x)))
plot(x,p1,type='l', ylab = "Probability",xlab='Dose')
lines(x,pp,lty=2)
lines(x,pc,lty=5)
```
```{r}
x <- seq(-2,8,0.2)
modl<-mod1
pl <- ilogit(modl$coef[1]+modl$coef[2]*x) 
pp <- pnorm(modp$coef[1]+modp$coef[2]*x) 
pc <- 1-exp(-exp((modc$coef[1]+modc$coef[2]*x))) 
plot(x,pl,type="l",ylab="Probability",xlab="Dose") 
lines(x,pp,lty=2)
lines(x,pc,lty=5) 

```

fitted probabilities

logit is solid line

probit is dotted line

log-log is the dashed line



look at the 'upper tail' and 'lower tail ratios between the probit and the logit ratios

for upper tail:

its the ratio of the probit probability of success over the logit probability of success

the probit p is LOWer than the logit for low values, but as they get higher, they become similar


the probit PROBABILITY of FAILURE is the SAME as the logit for low values, but MUCH LOWER RATE OF FAILURE in the high regions
```{r}
matplot(x,cbind(pp/pl,(1-pp)/(1-
pl)),type="l",xlab="Dose",ylab="Ratio")
```


```{r}
matplot(x,cbind(pc/pl,(1-pc)/(1-
pl)),type="l",xlab="Dose",ylab="Ratio")
```

Same ratio for loglog/logit

we also see that they make different predictions at the tails

PROBLEM:

different link functions are more sensitive to SMALL VALUES or HIGH VALUES. Think about a study on a poison that is bad in low doses vs one that needs  a mega dose to be poisonous. You would want to use different link functions for this. 


DEFAULT CHOICE IS LOGIT:

ADVANTAGES:
-simpler math
-easier to interpret odds
-easer analysis of retrospectivly sampled data


## Estimation Problems


Estimation using Fisher scoring algorithm is fast.

PROBLEMS WITH FISHER:

-when convergence fails. could be due to problem in the dataset.

```{r}
data("hormone")
plot(estrogen~androgen, data=hormone, pch=as.character(orientation))
```

My try on plot

```{r}
ggplot(hormone, aes(x=androgen,y=estrogen, color=orientation))+geom_point()
```

fit a binomial model to see if the orientation can be predicted from the two hormone values. 

```{r}
modl<-glm(orientation~estrogen+androgen, family = binomial(), data=hormone)
```

PROBLEMS WITH CONVERGENCE


```{r}
summary(modl)
```
NOTE:

residual deviance is extremely small indicating a very good fit and yet none of the predictors are significant due to HIGH standard errors.

we see max number of Fisher Scoring iterations has been reached.

we see that the two groups are linearly separable so a perfect fit is possible. 

we can compute the line separating the groups corresponds to p=1/2, which is when the logit is zero

(a 50/50 probability when the logit function predicts 0 (not sure why this is))
```{r}
ggplot(hormone, aes(x=androgen,y=estrogen, color=orientation))+geom_point()+geom_abline(intercept=-84.5/90.2,slope=100.9/90.2)
```

use EXACT LOGISTIC REGRESSION. 


## Prediction and Effective Doses

Predict outcome for given values of the covariates (predictors?) 

For binary data, this is estimating probability of success. 

approx confidence intervals can be obtained with normal approximation. 


To get answer back in probability scale, you have to transform with the link function


```{r}
data(bliss)
modl<-glm(cbind(dead,alive)~conc, family=binomial(), data=bliss)
lmodsum<-summary(modl)
```


How to predict the response at a dose of 2.5

```{r}
x0<-c(1,2.5)
#this is a dot product
eta0<-sum(x0*coef(modl))
#dot product verison
paste(x0%*%coef(modl),eta0)
#makes sense since it is a linear sum


```
since this value is the log odds, i guess we take the ilogit of it? (inverse log function)

```{r}
ilogit(eta0)
```
there is a 64% chance of death at this dose


Compute a 95% confidence interval THE LONG WAY

```{r}
#the covariance matrix

#how the variables vary together

cm<-lmodsum$cov.unscaled

#standard error on the logit scale is then

se<-sqrt(t(x0)%*%cm%*%x0)

#so the CI on the probability scale is then 

ilogit(c(eta0-1.96*se,eta0+1.96*se))
```


Easier way to get the confint

```{r}
(a<-predict(modl,newdata = data.frame(conc=2.5),se=TRUE))
```

```{r}
ilogit(c(a$fit-1.96*a$se.fit,a$fit+1.96*a$se.fit))
```


NOTE:

there is no distinction possible between confints for a future observation and those for the mean response


Now try predicting on a Low dose

```{r}
x0 <- c(1,-5)
se <- sqrt(t(x0) %*% cm %*% x0) 
eta0 <- sum(x0*modl$coef) 
ilogit(c(eta0-1.96*se, eta0+1.96*se)) 
```

NOTE: that the upper limit is 100x the lower limit


Logistic regression is widely used for classification purposes.

Depending on whether probability > .5 the case may b classified as SUCCESS or FAILURE. 

cases where losses due to misclassification are not symetrical (disease diagnosis) critical values OTHER than .5 should be used. 


CREDIT SCORING:

when banks decide whether to make a loan, estimate probability that a given borrower will default. 


Logistic regression is one way in which this probability can be estimated using past financial daa 



When there is a single (continuous) covariate, or when other covariates are held fixed, we sometimes wish to estimate the value of x corresponding to a chosen probability. 

EX:

determine which dose, x, will lead to a probability of success=p.ED50 (effective dose where there will be a 50% chance of success)


When the objective is to kill the subject or determine toxicity (insecticide) the term LD50 (50% chance of lethal dose)

Other percentiles are also of interest


for a `logit` link function, we can set p=1/2 and then solve for x to find

$$\hat{ED50}=-\frac{\hat{\beta_0}}{\hat{\beta_1}}$$

using bliss data

```{r}
(ld50<- -modl$coefficients[1]/modl$coefficients[2])
```

to determine the standard error, use DELTA METHOD:

delta method approximates standard errors of transformation of random variables using 1st order taylor approximation. 

Regression coefficients are themselves random variables, so we can use delta method to approximate standard errors of their transformations. 


COULD ALSO USE BOOTSTRAPPING


general expression of $g(\hat{\theta})$ for multivariate $\theta$


$$var~g(\hat{\theta})\approx g'(\hat{\theta})^T~var\hat{\theta}g'(\hat{\theta})$$


predicted variable is 2 for ld50



so we need to get the vector of partial derivatives of our function G(B) and the covariance of B


I guess the partial derivatives for this are 
$-1/\beta_1$
and 
$\beta_0/\beta_1^2$

so the gradient vector is
```{r}
dr <- c(-1/modl$coefficients[2],lmod$coefficients[1]/modl$coefficients[2]^2) 
```


and the $var\theta$ is the covariance matrix of the function

```{r}
varg<-lmodsum$cov.unscaled
```


so the standard error is

THIS ISNT THE SAME AS THE BOOK
```{r}
sqrt(dr%*%lmodsum$cov.un%*%dr)[,]
```


NEED TO FINISH THIS


other levels may be considered to find effective dose x_p

$$x_p=\frac{logit(p)-\beta_0}{\beta_1}$$
Use mass package to get effective dose at a probability

```{r}
library(MASS)
dose.p(modl,p=c(.5,.9))
```
you can get confint from here


## Overdispersion


if binomial GLM model specification is correct, we expect that the residual deviance will be approximately distributed on chisq with the appropriate degrees of freedom.


If we observe that the deviance is much larger than expected if the model were correct, we have to determine what aspect of the model specification is incorrect


MOST COMMON:

wrong structural form for the model

not included the right predictors, or not transformed and combined in the right way. 


When there are only one or two predictors, it is feasible to explore the model space throroughly and be sure there isnt a superior model formula


Large deviance could come from a small number of outliers

(check with diagnostics)

sparse data can also lead to large deviances (use the comparing deviances method)


Binomial distribution arises when probability of success is INDEPENDENT AND IDENTICAL for each trial

IF THIS ASSUMPTION IS BROKEN the variance may be large. This is *overdispersion*

when variance is less due to the wrong assumptions, this is *underdispersion*

EXAMPLE:

Shuttle disaster:

the position of the O-ring on the rocket may have some effect but wasn't recorded, so it can't be a predictor.

Heterogeneity (groups within the data) can also result from clustering. 

EXAMPLE:

population is divided into clusters, so when you sample, you are actually getting a sample of clusters


sample size be m, and cluster size be k. 

number of clusters be l=m/k 

number of successes in cluser i be $Z_i\sim B(k,p_i)$

suppose p_i is a random variable such that 

$Ep_i=p$ and $var~p_i=\tau^2p(1-p)$

MATH

conclusion:

the variance of Y is related to 1+(k-1) which is greater than 1, so as the cluster size increases the variance increases *overdispersion*

Overdispersion can result from dependence between trials.

If the response has a common cause, (disease influenced by genes), the responses will tend to be positively correlated.

EXAMPLE:

subjects in trials may be influenced by other subjects.


If food supply is limited, probability of an animal may be INCREASED by death of others. This would result in UNDERDISPERSION


best way to model overdispersion is to introduce an additional dispersion parameter$\sigma^2$


in the standard binomial case
$$\sigma^2=\Phi = 1$$

(makes sense since there are no groups k=1 so 1+(k-1)=1)

we now let dispersion parameter vary and estimate using the data.

(notice the similarity to linear regression (not sure what that is))

$$\sigma^2=\frac{X^2}{n-p}$$


EXAMPLE:

boxes of trout eggs were buried at 5 stream locations and retrieved 4 times. specified by the number of weeks after original placement.

number of surviving eggs was recording (box not returned to stream)
```{r}
data(troutegg)
head(troutegg)
```

create crosstab of the data

```{r}
troutegg%>%arrange(location)%>%relocate(location,period,survive,total)
```

notice that in one case all eggs survive, while in another none survivie

Fit a binomial GLM for the two main effects

```{r}
bmod<-glm(cbind(survive,total-survive)~location+period, family=binomial(),troutegg)
bmod
```
```{r}
pchisq(64.5,12, lower=FALSE)
```
really small p-value suggest model doesnt fit well


before checking for *overdispersion* eliminate other potential explanations

SPARSENESS:

100 eggs per box, not sparse

OUTLIERS:
```{r}
library(ggResidpanel)
resid_interact(bmod, plots='all')
```

It says half normal distribution of residuals is good for looking for outliers ( i can see some outliers in the Cooks distance 14,15,19,20)

It says use half-normal plot on residuals

*half-normal probability plot*

what are the important factors (including interactions)?


estimated effect of a given effect or interaction and its rank relative to other effects is given by least squares estimation (which variable minimizes the sum of the residuals the most)

half-normal probability plot is a graphical tool that uses ordered estimated effects to help assess which factors are important and which unimportant

vertical axis:

absolute value of the estimated effect for the main factors and available interations

horizontal axis:
theoretical order statistic medians 

estimated effect of unimportant factors will be near 0,0

important factors will be far of the line



```{r}
halfnorm(residuals(bmod))
```

this says that 15 and 20 are very impactful on the data

```{r}
rbind(troutegg[15,],troutegg[20,])

```


also check whether predictors are correctly expressed by plotting the `empirical logits`

$$log(\frac{y+1/2}{m-y+1/2})$$



**empirical logits**

adds 1/2 to each proportion (successes and failures)

done because logit function goes to $-\infty$ for 0 successes and $\infty$ for 0 failures

empirical logits provides a good normalizing transformation for proportions


```{r}
elogits <-
log((troutegg$survive+0.5)/(troutegg$total-
 troutegg$survive+0.5))
with(troutegg,interaction.plot(period,location,elogits)
) 
```

```{r}
df<-data.frame(elogits=elogits,period=troutegg$period,location=troutegg$location)
ggplot(df,aes(x=period,y=elogits,group=location,color=location))+geom_point()+geom_smooth(se=FALSE, method="loess", span=0.5)+ggtitle('interaction plot')
```

there is no obvious sign of large interactions in the interaction plot

HOW TO READ INTERACTION PLOT:

*lines parallel*:
no interaction

*lines antiparallel*:
there is an interaction


The lines in the interaction plot are mostly parallel, so we say there is no interaction between the groups, so there is no evidence that the linear model is inadequate. 


We do not have any outliers ( i dont know why they are saying this, the cook's distance seems to have high leverage points)


Since we have eliminated these causes, we can put the blame on dispersion.

### possible reasons for dispersion

inhomogeneous trout eggs (different types of trout eggs?) variation in the experimental procedures or unknown variables affecting survival.


*estimate dispersion parameter*

dispersion parameter= residuals^2/degrees_of_freedom
```{r}
(sigma2<-sum(residuals(bmod,type='pearson')^2)/12)
```


We can see that this is substantially larger than one, as it would be in a standard binomial GLM. 


Make an F-test on the predictors

*F-test*

(test that uses the F distribution)

(f-test to compare two variances)
drop1 runs a model without a term to see if they are different

```{r}
drop1(bmod,scale=sigma2,test='F')
```

from the p-values, we see that both terms are significant. 

we specify `scale=sigma2` using our estimation. If this argument is omitted, the deviance will be used in estimation of the dipersion param. (doesnt make a lot of difference in this example)

```{r}
drop1(bmod,test='F')
```
(the AICs are higher, i guess because there is unexplained deviance)



NO GOODNES-OF-FIT test possible because we have a free dispersion parameter.



we can use the dispersion parameter to scale up the estimates of the standard error:

```{r}
summary(bmod, dispersion=sigma2)
```
summary without dispersion
```{r}
summary(bmod)
```
NOTE: this dispersion parameter makes the std. error bigger (i guuess because we are admitting there is unexplianed variance?)



## Matched Case-Control Studies


case-control study:

try to determine the effect of certain risk factors on the outcome. 

WE UNDERSTAND that there are other confounding variables that may affect the outcome.

one approach is to measure or record them, and include them in the logistic regression model as appropriate and thereby control for their effect. BUT this method requires that we model these confounding variables with the correct functional form (which is hard)

in a *matched case-control study* we match each case (disease person, defective object, success, etc) with one or more controls that have the same or similar values of some set of potential confounding variables. 

EXAMPLE:

56 year old hispanic male case. Try to match him with some number of controls who are also 56- year-old Hispanic males. called a *matched set*

The more confounding variables one specifies, the more difficult it will be to make matches. 

Loosening the matching requirements (accepting a wider range of ages) might be necessary

matching can also control for confounders that are difficult to measure. 


eX:

suspect an environmental effect on the outcome. HOWEVER it is difficult to measure exposure, particularly when we may not know which substances are relevant.

we could match subject based on their place of residence, this could adjust for environmental effects

DISADVANTAGES:

hard to form matched sets. 

YOU LOSE the possibility of discovering effects of the variables used to determine the matches

EX:

if we match on sex, we cant investigate a sex effect. 
matched sets are farther from a random sample of a population of interest. 

hard to generalize from a matched set to a whole population. 



