---
title: "Count Regression"
author: "Jack Wright"
date: "10/27/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
library(tidyverse)
library(faraway)
```

## Count Regression

when response is a count (positive integer) use count regression


WHEN COUNT IS BOUNDED

use binomial regression.

COUNT IS SUFFICIENTLY LARGE FOR NORMAL APPROXIMATION

normal linear model used



## Poisson regression

IF Y IS Poisson with $\mu>0$

### Poisson distribution

discrete probability distribution

expresses the probability of a given number of events occuring in a fixed interval of time or space IF these events occur with a known constant mean rate AND independently 


EX:

call center receives an avg of 180 calls per hour 24 hours a day 

-calls are independent (recieving one does not change when the next might come in)

NUMBER OF CALLSER PER MINUTE has a poisson probability distribution

most likely are 2,3, but 1 and 4 are also likely. 

small probability of 0, very small of 10.




$$P(Y=y)=\frac{e^{\mu}\mu^y}{y!}$$



EY=var Y = $\mu$

Poisson distributino arises naturally

1.

count is some number out of a possible total. *model this with a binomial* BUT for small success probabilities and LARGE totals, Poisson is a good approximation. 

EX:

modeling incidence of rare forms of cancer. Number of people affected is a small proportion of the population given a geographical area.

if $\mu=np$ while $n\rightarrow\infty$ then B(n,p) is well approximated by $Pois(\mu)$

NOTE:

for a small p, $logit(p)\approx log(p)$

so use of Poisson with a log link is comparable to the binomial with a logit link

2.

Probability of occcurrence of an event in a given time interval is proportional to the length of that time interval and independent of the occurence of other events.
( i guess as the time interval increases so does the probability of getting a call?)

Number of events in any specified time interval will be Poisson distributed

EX:

modeling the number of incoming telephone calls to a service center or the number of earthquakes.

rate of incoming telephone calls is likely to vary with time of day, while the timing of earthquakes are unlikely to be completely independent. However it might be a good approximation



3.

Poisson distributions also arise natruarlly when the time between events is independent and identically exponentially distributed

Count the number of events in a given time period.

If the count is the number falling into some level of a given category, then a multinomial response model or categorical data analysis should be used. 

EX:

counts of people who haev O,A,B, or AB blood and are interested in how this relates to race or gender, then a straight Poisson regression will not work

NOTE:

Sum of Poisson random variables is also a Poisson. 

EX:

$Y_i=Pois(\mu_i)$ for i=1,2.. and are independent.

Then 

$\sum_iY_i\sim Pois(\sum_i \mu_i)$

Sum of results of poisson equal to the sum of the mus?)

Useful because sometimes we only have access to aggregated data.

Assume individual-level data is Poisson, then the summed data and the Poisson regression can still be applied.



EX:

30 galapagos islands, have count of the number of species of tortoise found on each island and the number that are endemic to that island. 


5 geographic variables for each island. 

model using normal linear regression


```{r}
library(GGally)
data(gala)
gala<-gala[,-2]
ggpairs(gala)
```

I might do some transformation here, but the book doesn't


```{r}
library(ggResidpanel)
modl<-lm(Species~.,gala)
resid_panel(modl,plots = 'all')
```

we see non constant variance in residuals vs fitted

check the box cox method

```{r}
library(car)
boxCox(modl)
```

```{r}
library(car)
#create test data frame with +.01 to Scruz to get rid of zeros

gala2<-gala%>%mutate(Scruz=Scruz+.01)
powerTransform(gala2)
```

looks like these are saying the power transform for species is closest to .5 (or square root)

take the square root of the predictor


```{r}
gala2<-gala2%>%mutate(Species=sqrt(Species))
ggpairs(gala2)
```

```{r}
modt<-lm(Species~.,gala2)
```



```{r}
resid_panel(modt)
```

residuals look better


```{r}
summary(modt)
```
fairly good R^2 fit. HOWEVER, achieved this by transforming the response, which makes interpretation more difficult. 

ALSO:

response values are quite small, which makes us question validity of the normal approximation. 


TRY AGAIN WITH POISSON

Suppose we have count responses Y_i and want to model in terms of a vector of predictors x_i


if $Y_i\sim Pois(\mu_i)$

we need to link $\mu_i$ with $x_i$

Use a linear combination of $x_i$ to form the linear predictor

$\eta_i=x_i^T\beta$

since we need mu to be larger than zero, ensure this by using a log link function


$log\mu_i=\eta_i=x_i^T\beta$


MATH

log likelihood after differentaiation for B

$$X^Ty=X^T\mu$$



normal equations for the least squares estimate of $\beta$ in normal linear models take the same form (we can use the least squares estimates were used to) 

when we set $\hat{\mu}=X\hat{\beta}$ (this is only true for he logit link function)

There is no explicit formula for beta in the Poisson (or binomial) regression so you have to fit it


```{r}
gala_glm<-gala
#gala<-gala[,-2]

modp<-glm(Species~.,family=poisson,gala_glm)
summary(modp)
```

using same arguments for binomial, we develop deviance for the Poisson regression

## G-statisitc

deviance for the Poisson regression


use the same goodness-of-fit metrics for Poisson

```{r}
pchisq(716.85,24,lower.tail = FALSE)
```

This says model is not a good fit, because we are comparing the model to the saturated model, p-value should be high.

check the residuals with half norm to see if a large deviance can be explained by an outlier

```{r}
halfnorm(residuals(modp))
```

book says half normal plot doesnt show outliers

COULD BE STRUCTURAL FORMof the model needs some improvement

(you could mess around but the book says "there is little scope for improvement")

They also note that the *proportion of deviance explained by the model* is about the same as the linear model

```{r}
#formula for R^2_dev
1-717/3510

```

For a Poisson distribution the mean is equal to the variance

INVESTIGATE

hard to estimate variance for a given value of the mean but there is a crude approx

$$(y-\hat{\mu})^2$$

plot this for all fitted values

```{r}
library(latex2exp)
ggplot(gala, aes(x=log(fitted(modp)),y=log((Species-fitted(modp))^2)))+geom_point()+xlab('$\\hat{\\mu}$')+geom_abline(intercept = 0,slope = 1)
```


The line represents the mean  = to the variance (since they are the same in a Poisson)


We see that the variance is proportional (sort of follows the line) but LARGER than the mean.


When the variance assumption for a Poisson regression model is broken (the variance is not equal to the mean), but the link function and choice of predictors is correct, this means *the estimates of $\beta$ are consistent, but the standard errors will be wrong*



The Poisson distribution only has one parameter so it is not very flexible for empirical fitting purposes.

We can generalize by allowing ourselves a dispersion parameter.

Over- or underdispersion can occur in Poisson models


EX:

suppose the Poisson response Y has rate $\lambda$ which is itself a random variable. 


The tendency for a machine to fail may vary from unit to unit even though they are the same type of machine (i guess this would be lambda as a random variable)


model this by letting $\lambda$ be gamma distributed

(chisq is a special case of a gamma distribution, it has 2 paramters)

parameters in this case:

$$EY=\mu$$
$$\lambda = \frac{\mu}{\phi}$$



Recall that in Poisson mean =variance


The mean would be the same when we let lambda be gamma distributed, but when we try to set mean=variance we get

LOOKING AT NEW VARIANCE WITH GAMMA distributed LAMBDA

$$Y=\mu(1+\phi)/\phi$$

which obviously isnt equal to the mean mu

THIS CAUSES OVERDISPErSION



IF WE KNOW the specific mechanism, we could model the response as a negative binomial.

If the mechanism is not known we can introduce a dispersion parameter $\phi$

so that 


$$Y=EY(expected~value~of~Y)=\phi \mu$$

where $\phi=1$ for the regular Poisson

and $\phi>1$ is overdispersion

and 

$\phi<1$ is underdispersion


## Dispersion parameter estimation

$$\hat{\phi}=\frac{X^2(pearson~residual)}{n-p}=\frac{\sum_i(y_i-\hat{\mu_i})^2/\mu_i}{n-p}$$

ESTIMATE DISPERISION EXAMPLE:

```{r}
 (dp <-
sum(residuals(modp,type="pearson")^2)/modp$df.res)
```
(i guess n-p=1? not sure why)


## AER package dispersion_test

```{r}
library(AER)
AER::dispersiontest(modp)
```

*ANOTHER OPTION FOR DISPERSION TEST*

library(DHARMa)
sim_fmp <- simulateResiduals(fmp, refit=T)

```{r}
library(DHARMa)
sim_modp<-DHARMa::simulateResiduals(modp,refit=T)
DHARMa::testOverdispersion(sim_modp)
```


```{r}
DHARMa::plotSimulatedResiduals(sim_modp)

```

adjust the standard errors for the model

```{r}
summary(modp,dispersion=dp)
```

Notice that the estimation of the dispersion and the regression parameters is independent. (changing the dispersion doesnt change the regression parameter)

*undadjusted model*
```{r}
summary(modp)
```

## comparing Poisson models with overdispersion 

Use F-test rather than chisq


test the significance of each of the predictors relative to the full model

```{r}

 drop1(modp,test="F")
```
look at the p-values to determine significance of the variable


(same ones are deemed significant in this case)




## Rate Models

in a variable:

number of events observed may depend on the size (opportunities for events to occur)

EX:

number of burglaries reported in different cities,

observed burglaries will depend on number of households in the city

Size variable could also be time

EX:

number of customers served by sales worker, you need to take into account the amount of time worked



Could use a binomial response model

BURGGLAR EXAMPLE:

model the number of burglaries out of the number of households.

HOWEVER:

if the proportion is small, the Poisson approximation to the binomial is effective

In some examples, the total number of potential cases may not be exactly known.


Modeling rare diseases:

we may know the number of cases, but not have the precise population data.

BACK TO  BURGLAR:

some household may be affected more than once, (so it isnt binary binomial, or if you group them the ms are too small)

CUSTOMER SERVICE EXAMPLE:

size variable is not a count


ALTERNATIVE APPROACH, MODEL THE RATIO 

-could cause problems with normality and unequal variance, especially when the counts are small


EXAMPLE:

determine the effect of gamma radiation on the numbers of chromosomal abnormalities (ca) observed

the number(cells) in hundreds of cells exposed in each run differs

the dose amount(doseamt) and the rate(doserate) at which the dose is applied are the predictors of interest



RECALL:

xtabs is a contingency table, cross-classifying factors


```{r}
data(dicentric)
round(xtabs(ca/cells~doseamt+doserate,dicentric),2)
```

Try modeling the rate directly, we see that the effect of dose rate may be multiplicative

```{r}
dicentric<-dicentric%>%mutate(doserate=as.factor(doserate))
ggplot(dicentric,aes(x=doseamt,y=ca/cells,group=doserate,color=doserate))+geom_smooth()+scale_color_brewer(palette="Dark2")
```



```{r}
data(dicentric)
lmod <- lm(ca/cells ~ log(doserate)*factor(doseamt),
dicentric)
summary(lmod)
```

look at adjusted R^2, the model fits really well

BUT look at the diagnostics

```{r}
resid_panel(lmod)
```

Notice the variance changes as the predicted value increases


Might prefer an approach that directly models the count response. 


```{r}
ggpairs(dicentric)
```

Need to use the log of the number of cells because we expect this to have a multiplicative effect on the response:

```{r}
dicentric$dosef<-factor(dicentric$doseamt)
pmod<-glm(ca~log(cells)+log(doserate)*dosef,family=poisson,dicentric)

summary(pmod)
```

We can relate this poisson model with a log link back to a linear model for the ratio response

RECALL:

the log link function between linear regression and Poisson modeling

$log\mu_i=\eta_i=x_i^T\beta$

so our mu (expected value) is ca/cells

$$log(ca/cells)=X\beta$$
and

$$log(ca)-log(cells)=X\beta$$
so our model

$$log(ca)=log(cells)-X\beta$$


which is why we are using log(cells) as a predictor even though it is part of our rate

SO we are modeling the rate of chromosomal abnormalities while still maintaining the count response for the Poisson model

known as a *rate model*



LOOKING AT COEFFICIENT OF CELLS:

we see that 1.025 is very close to one. 

This suggests fitting a model with the coefficient fixed as one. 

we are modeling the rate of chromosomal abnormalities while still maintaining the count response for the Poisson model. 

*the reason why the response variable isnt logged is because of the LINK FUNCTION which in Poisson is already a log, so if you logged it, it would be double logged.


use offset(log(cells)) which i guess will just nudge our model over to that slope. 
```{r}
rmod <- glm(ca ~ offset
(log(cells))+log(doserate)*dosef,
 family=poisson,dicentric)
summary(rmod) 
```


the coefficients are only slightly different from the previous model


check residuals

```{r}
resid_panel(rmod)
```

The residuals fit a lot better. 

from this model we can say that the doserate is different depending on the overall dose

we also see that a high dose, delivered quickly has a greated combined effect than the main effect estimates would suggest



## Negative Binomial

WIKIPEDIA:

Negative binomial is  a distribution that models the number of successes in a sequence of independent and identically distributed Bernoulli trials BEFORE a specified numbe of failures occurs. 

BOOK:

given a series of independent trials each with probability of success p, let Z be the number of trials until the kth success. 



negative binomial can arise naturally in several ways.

Envision a system that can withstand k hits. 

probability of a hit in a given time period is p

NEGATIVE binomial also arises from the generalization of the poisson where the parameter $\lambda$ is gamma distributed. 


Negative binomial also comes up as a limiting distribution for *urn schemes* that can be used to model contagion.
p=probability of success
Z= number of trials before kth success

$$P(Z=z)=\begin{pmatrix}z-1\\k-1
\end{pmatrix}p^k(1-p)^{z-k}$$
URN SCHEME:

i guess the urn problem where you pull green and red marbles?


if we let $Y=Z-k$ (number of trials before kth succes-kth success?)
and $p =(1+\alpha)^{-1}$

relationship between p and alpha

```{r}
library(kableExtra)
p<-seq(0,1,.1)
alpha<-(1-p)/p
df<-data.frame(probability=p,alpha=alpha)
df%>%kbl()%>%kable_classic_2()
```
i guess alpha gets lower when probability gets higher
```{r}
ggplot(df, aes(x=probability, y=alpha))+geom_smooth(method='loess')
```


$$P(Y=y)=\begin{pmatrix}y+k-1\\k-1
\end{pmatrix}\frac{\alpha^y}{(1+\alpha)^{y+k}}$$

This makes the expected value of the distribution

$$EY=\mu=k\alpha$$

and the variance
$$Var~Y=k\alpha+k\alpha^2=\mu+\mu^2/k$$


link function

$$\eta=x^T\beta=log(\frac{\alpha}{1+\alpha})=log(\frac{\mu}{\mu+k})$$


k is fixed and determined by the application or as an additional parameter to be estimated. 


EXAMPLE:

ATT ran an experiment varying 5 factors relevant to a wave-soldering procedure for mounting components on a PCB. 

The response variable, skips, is a count of how many solder skips appeared to a visual inspection 


```{r}
data(solder)
modp<-glm(skips~.,family = poisson, data=solder)
summary(modp)
```

full model has deviance of 1829 on 882 degrees of freedom

```{r}
anova(modp, test='Chisq')
```
```{r}
pchisq(1829,882, lower.tail = FALSE)
```

low p means we reject our model, it is not a good enough fit


Try adding in interaction terms

```{r}
modp2<-glm(skips~(Opening+Solder+Mask+PadType+Panel)^2,family=poisson, data = solder)
summary(modp2)
```
```{r}
pchisq(1069,790,lower.tail = FALSE)
```

better, but not good enough to say the model fits
```{r}
halfnorm(residuals(modp2))
```


says there are no major outliers



Alternative model is the NEGATIVE BINOMIAL

functions for fitting come from MASS package

we use k=1 to demonstrate, but there is no reason to

```{r}
library(MASS)
modn<-glm(skips~.,negative.binomial(1),solder)
summary(modn)
```

we could experiment with different values of k, but the more direct way is allowing k to vary and be estimated

```{r}
modn<-glm.nb(skips~.,solder)
summary(modn)
```

we see $\hat{k}=4.397$ with a standard error of .495


compare negative binomial models using usual inferential techniques



## Exercises

1.

The dataset discoveries lists the numbers of “great” inventions and scientific
discoveries in each year from 1860 to 1959. Has the discovery rate remained constant
over time? 


```{r}
data("discoveries")
print(discoveries)

plot(discoveries)
```
```{r}
years<-seq(1860,1959,1)
df<-data.frame(years=years, disc=discoveries)
```



its count data, so I guess ill try a poisson

```{r}
#make a list of potential models

discmod_list<-list()

disc_p<-glm(disc~years,family=poisson,data=df)
summary(disc_p)

```
looks ok

```{r}
pchisq(157,98,lower.tail=FALSE)
```

```{r}
summary(disc_p)$coef['years','Pr(>|z|)']
```

model doesnt work, maybe try a quadratic?

```{r}
disc_p_quad<-glm(disc~poly(years,2),family=poisson,data=df)
summary(disc_p_quad)
```

both highly significant. residual deviance is a little better AIC is a little lower

```{r}
pchisq(132,97,lower.tail=FALSE)
```

still not good enough

```{r}
ggplot(df,aes(x=years,y=disc))+geom_smooth()
```


check if model has outliers with halfnorm

```{r}
halfnorm(residuals(disc_p))
```

i think this is ok


examine variance


$$(y-\hat{\mu})^2$$

plot this for all fitted values



```{r}
ggplot(df, aes(x=log(fitted(disc_p)),y=log((disc-fitted(disc_p))^2)))+geom_point()+geom_abline(intercept = 0,slope = 1)
```

maybe there is dispersion? they dont look like they fit the line that well. lets calculate

```{r}
(dp <-
sum(residuals(disc_p,type="pearson")^2)/disc_p$df.res)
```

look at model again with dispersion calculated in

```{r}
summary(disc_p,dispersion=dp)
```

```{r}
pchisq(157,98,lower.tail=FALSE)
```


doesnt look good...


The example i found online
```{r}
data(discoveries)

### The dataset discoveries lists the numbers of \great" inventions 
### and scientific discoveries in each year from 1860 to 1959. Has the 
### discovery rate remained constant over time?

year=1860:1959
g=glm(discoveries ~ year, family="poisson")
summary(g)$coef['year','Pr(>|z|)']
```
# When comparing Poisson models with overdispersion, an F-test rather
# than a chi-squared test should be used
# The drop1 function tests each predictor relative to the full. 
```{r}
drop1(g, test="F")
```
# p-value<.05, We see that year as predictor is significant.
```{r}
dp=sum(residuals(g, typp="pearson")^2)/df.residual(g); dp
(g$null.deviance - g$deviance)/dp
```
# dp>1: evidence that p-value in regression was overestimated
# since didn't scale parameter (overestimate p-value means
# that the p-value was smaller than it should be)
```{r}
summary(g, dispersion=dp)
```
# p-value <0.05 when estimated dispersion parameter used.
# Effect of year stat significant, which implies rate of 
# discovery not constant over time




Try a negative binomial model

```{r}
modn<-glm.nb(disc~.,df)
summary(modn)
```


```{r}
pchisq(108,98,lower.tail=FALSE)
```

model looks good, years is significant and (nonzero? its pretty close to zero) 

i guess we check if the null deviance-residual deviance is significantly differnt

```{r}
pchisq(114-108,1,lower.tail = FALSE)
```

The null model is ok. but the lambda means that the rate of success changes i think


2.

The salmonella data was collected in a salmonella reverse mutagenicity assay. The
predictor is the dose level of quinoline and the response is the numbers of revertant
colonies of TA98 salmonella observed on each of three replicate plates. Show that a
Poisson GLM is inadequate and that some overdispersion must be allowed for. Do not
forget to check out other reasons for a high deviance.


```{r}
data(salmonella)
ggplot(salmonella,aes(x=dose,y=colonies))+geom_point()
```

looks quadratic or something


```{r}
modp<-glm(colonies~dose,family=poisson,data=salmonella)
summary(modp)
```




```{r}
pchisq(75.8,16,lower.tail=FALSE)
```


```{r}
pchisq(55.5,15,lower.tail=FALSE)
```


```{r}
resid_panel(modp)
```

trying one i found online

```{r}
sal.fit <- glm(data= salmonella, formula = colonies ~ factor(dose), family = poisson)
```


```{r}
summary(sal.fit)
```

trying the `display` function this guy uses
```{r}
library(arm)
display(sal.fit)
```

```{r}
meanv<-tapply(salmonella$colonies, salmonella$dose, function(x)c(mean = mean(x),variance = var(x)))
```
he looks at the mean and variance of colonies by the factor level `dose`


```{r}
#turn list of lists into dataframe
df<-as.data.frame(do.call(rbind,meanv))
df<-cbind(dose = rownames(df),df)
rownames(df)<-1:nrow(df)
```



```{r}
ggplot(df,aes(x=mean,y=variance))+geom_smooth()
```


he wants to show the general trend that as mean increases, so does variance, which is generally true except for 35. maybe this is the dispersion?

he says that when you look at this graph  the num colonies as a function of dose is not `monotonic` (it doesnt always move in the same direction)



he looks at the p-value of the deviance
```{r}
pchisq(33.496,12,lower.tail=FALSE)
```


he says we should look at the log scale, since we are fitting a log linear model

```{r}
sal.fit1 <- glm(data= salmonella, formula = colonies ~ dose, family = poisson)
summary(sal.fit1)
```

Now he runs it without the groups, 

says look at fit of residuals

```{r}
plot(sal.fit1,which =1 )
```

looks like a quadratic in the residuals



says lack of fit is also evident if you plot the fitted line in the data

```{r}
plot(colonies ~ dose, data = salmonella)
lines(x = salmonella$dose, y= predict.glm(sal.fit1, type = "response") )
```


tries some polynomial fits

```{r}
sal.fit2 <- glm(colonies ~ poly(dose, 4),data = salmonella, family = poisson(link = "log"))
summary(sal.fit2)
```
```{r}
pchisq(34.98,13,lower.tail = FALSE)
```
still no good


```{r}
plot(sal.fit2,which =1 )
```

says overdispersion still exists, so use quasipoisson

```{r}
sal.fit3 <- glm(colonies ~ poly(dose, 4),data = salmonella, family = quasipoisson(link = "log"))
summary(sal.fit3)

```
This quasipoisson seems to get the dispersion for you. Ill check if its the same as when we calc it


```{r}
(dp <-
sum(residuals(sal.fit2,type="pearson")^2)/sal.fit1$df.res)
```
The dispersions seem to be close, let me try to do the summary thing with the non quasi version

```{r}
summary(sal.fit2,dispersion=dp)
```

This way doesnt seem to downgrade the p values of the predictors as much



3.

The ships dataset found in the MASS package gives the number of damage incidents
and aggregate months of service for different types of ships broken down by year of
construction and period of operation. Develop a model for the rate of incidents,
describing the effect of the important predictors. 



RATE of incidents, means incidents per month of service

Necessarily thi
```{r}
library(MASS)
ships<-MASS::ships
ships<-relocate(ships, incidents)
summary(ships)
```

There are ships with 0 months of service, exclude those

```{r}
ships<-ships%>%filter(service>0)

```

To get a `rate`, we want the dependent variable to be incidents/service. Note the residuals from this model

```{r}
lmod<-lm(data=ships, incidents/service~type+year+period)
resid_panel(lmod)
```
Note how the variance expands as the predicted values increase. This is because var=mean in count data. LM is not appropriate

Because we are modeling a rate statistic and the link function is a logarithm, we are able to pull out the denominator and model only on the numerator and set the denominator as an offset.





looks like a lot of incidents around zero

make some models

```{r}
glm_list=list()

glm_list[['All']]<-glm(
  incidents~type+year+period,offset = log(service),family = poisson,data=ships
)
glm_list[['quasi']]<-glm(
  incidents~type+year+period,offset=log(service),family=quasipoisson(),data=ships
)
#negative binomial if we infer that the possible range of accidents is infinite
glm_list[['nb']]<-glm.nb(
  incidents~.,ships
)


```


```{r}
lapply(glm_list,
       function(x)anova(
         object=x,
         test='Chisq'
       ))
```

```{r}
lapply(
  glm_list,
  FUN=AIC
)
```

```{r}
summary(glm_list[['All']],dispersion=2.43)
```
```{r}
modnb<-glm_list[['All']]
ggplot(ships,aes(x=incidents,y=modnb$fitted.values))+geom_point()+geom_abline(slope=1,intercept=0)
```

check significance of predictors

```{r}
anova(glm_list[['quasi']],test='F')
```

```{r}
ships2<-ships
glm1 <- glm(incidents ~ type + year + period, 
    family = poisson(link = "log"), data = ships2, offset = log(service))
summary(glm1)
par(mfrow=c(2,2))
resid_panel(glm1)
```

quasi likelihood allowing for dispersion

```{r}
glm2 <- update(glm1, family = quasipoisson(link = "log"))
summary(glm2)
resid_panel(glm2)

```

check significance of predictors

```{r}
anova(glm2, test = "F")
```


NOTE: in this model we use log(service) as a predictor, its coefficient should be near 1


```{r}
# check if damage is roughly proportional to service
glm3 <- glm(formula = incidents ~ type + year + period + log(service),
  family = quasipoisson(link = "log"), data = ships2)
summary(glm3)
```

