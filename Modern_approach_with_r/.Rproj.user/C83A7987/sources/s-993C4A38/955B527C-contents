---
title: "ch_6"
author: "Jack Wright"
date: "9/28/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



Diagnostics and Transformations for Multiple Linear Regression

## Regression Diagnostics for Multiple Regression


look at regression diagnostics


1.

Determine whether proposed regression model is a valid model

(fits the data)

*standardized residuals*

*fitted values*

*marginal plot models*

2.

determine data points perdicotr values have an unusually large effect on the estimated regression model 
*leverage points*

3.

determine *outliers*


4.

assess effect of each predictor variable on response variable using
*added variable plots*

5.

assess *colinearity* among predictor variables using *variance inflation factors*


6.

examine assumption of constant error variance is reasonable. 

7.

examine if data are correlated over time



## Leverage points in Multiple Regression

leverage measures extent to which the fitted regression model is attracted to a data point.

interested in relationship between fitted values and actual values

Hat matrix


BASIC IDEA:

pre multiplying the leverage matrix by the actual values gives the fitted values

### rule for identifying leverage points

classify ith point as a point of high leverage in a multiple linear regression model with p predictors if 

h_ii > 2* average leverage




## Properties of residuals in multiple regression


expected value of the vector of residuals is zero ( why?)


variance of the vector of residuals is 

$$Var(\hat{e}|X)=\sigma^2(I-H)=H$$


### Standardized residuals

Varrianc of the error of the ith point = sigma^2[1-leverage of that point]


standard residual r_i

$$r_i = \frac{\hat{e}_i}{s\sqrt{1-h_{ii}}}$$

standardized residual = error of that point / standard error( average error of estimate) times square root of 1- leverage of the specific point

BASICALLY:

actual error value over scaled standard error depending on the leverage



we label outliers as points that fall outside -2 and 2 on standardized scale



## Using residuals and standardized residuals for model checking


when a valid model has been fit, a plot of standardized residuals "r_i" against any predictor or linear combination of predictors (such as fitted values) will have following features:

1.

random scatter of points on horizontal axis

2.

constant variability


ANY PATTERN MAKES MODEL INVALID


## MENU PRICING EXAMPLE

```{r}
library(tidyverse)
library(here)
file<-here('data','nyc.csv')
dat<-read.csv(file)
```



consider full model

$$Y=\beta_0+\beta_1 x_1 + \beta_2 x_2 +\beta_3 x_3 + \beta_4 x_4$$


look at pair plot

```{r}
library(GGally)
ggpairs(dat%>%select(c(Food, Decor, Service)))
```

Predictors seem to be linearly related

look at plots of standardized reesiduals for each one

```{r}
lm.model<-lm(Price~.,data=dat%>%select(-c(Restaurant,Case)))
library(ggResidpanel)
resid_panel(lm.model, plots = 'all')
```

## Examples where you cant tell whats wrong from the residuals

```{r}
file<-here('data','nonlinearx.txt')
dat<-read.table(file, header=TRUE)

ggpairs(dat)
```

look at model
```{r}
lm.model<-lm(y~.,data=dat)
library(ggResidpanel)
resid_panel(lm.model, plots = 'all')
```


we can see the conditions do not hold, but we can't say what part of the model is wrong


look at individual residuals
```{r}
resid_xpanel(lm.model)

```


Suppose we are considering introduction of additional predictor variable Z into a general linear regression


$$Y=X\beta +Z\alpha +e$$


we are interested in alpha (regression coefficient measuring effect of Z on Y)

*added variable plot* for predictor variable Z enables us to visually estimate alpha. 


### added variable plot

plot residuals from model on vertical axis


plot (something) residuals???? something missing in textbook


gives the part of Z that is not predicted by X

THE EFFECTS DUE TO X are removed from both axes.. 


## example of added variable plot

individuala plots that display the relationship between a response variable and one predictor variable in multiple regression controlling for the presesnse of other predictor variables in the model


```{r}
library(car)
model<-lm(mpg~disp+hp+drat, data=mtcars)
summary(model)
```
visualize relationship between response variable 'mpg' and each individual predictor variable in the model, produce added variable plot using avPlots()

```{r}
avPlots(model)
```
how to iinterpret

x axis displays single predictor variable and y axis displays response
HOLDING VALUE OF ALL OTHER PREDICTORS CONSTANT

blue line shows association between predictor variable and response variable while holding value of all other predictor variables constant


points are labelled in each plot repressent the two observations with the largest residuals and two obs with largest partial average

PARTIAL AVERAGE:

using a constant for B is constraining, since different data points will have different effects on Y, so the largest partial average is basically saying theese points have the largest betas, if you were considering beta as a random variable with a distribution instead of a constant


these plots allow us to visualize the relationship between each individual predictor variable and the response variable


#menu pricing example again


```{r}
library(tidyverse)
library(here)
file<-here('data','nyc.csv')
dat<-read.csv(file)
```


```{r}
ggpairs(dat%>%select(-c(Restaurant,Case)))
```

look at price vs predictor. 

shortcoming is that it looks at effect of a given predictor on Price, ignoring the effects of the other predictors on Price. 


look at added variable plots to separate these coliniaritys

```{r}
model<-lm(Price~., data=dat%>%select(-c(Restaurant,Case)))
avPlots(model)
```

This effect can also be seen in the p-value (staistical significance of the variable) in the summary of the model 


Look at Price vs Food

The two points 117 and 168 are identified as having a LARGE INFLUENCE on the model and should be investigated

case 117 is a restaurnat called Veronica, which has very  low decor and service scores, but a high food score, and the price is high. 

168 is called Gennaro, which has low Decor and Service but a high food score for a low price. 

Gennaro is described by Zagats the following way

Upper Westsiders gennar-ally gush over this “unassuming” cash-only Italian “gem”, citing
“sophisticated” preparations at “bargain” prices; to cope with “awful lines”, “crapshoot” service
and a room “packed tighter than a box of pasta”, go at off-hours.




## Transformations

used to 

-overcome problems due to nonlinearity

-estimate percentage effects


## Transformations to overcome nonlinearity

-inverse response plots
-box-cox procedure


A:

response variable needs to be transformed

B:

predictor variable needs to be transformed

C:

both 



## Methods for estimating reverse function for transforming model


EXAMPLE:

defective parts on assembly line dataset

```{r}
file<-here('data','defects.txt')
dat<-read.table(file,header=TRUE)
```

take a first look

```{r}
ggpairs(dat%>%select(Defective,Temperature,Density, Rate))
```

look at plots of standardized resisudals against each predictor

```{r}
summary(model<-lm(Defective~Temperature+Density+Rate, data=dat%>%select(-Case)))

```
```{r}
resid_panel(model, plots='all')
```

resid vs fitted looks more quadratic than linear


residuals dont look random


Since the fitted vs predict looks quadratic, consider a transformation of Y


## Inverse response plots

true regression looks like this

$$Y=g(\beta_0+\beta_1 x_1....etc)$$
where g is an unknown function. 


you can take the inverse of both sides to get

$$g^{-1}(Y)$$


if we knew the betas, we could discover the shape of G by plotting Y on horizontal and B0_B1x1+B2x2... on the vertical axis


these can be estimated on the scatter plot

called *inverse response plot*

EXAMPLE:

since we found that the predictors at least approximately linearly corrolated

we mark 3 power curves

$\hat{y}=y^{\lambda},for y=0,.44,1$
```{r}
df=data.frame(y_hat=model$fitted.values,Defective=dat$Defective)

ggplot(df, aes(x=Defective, y=y_hat))+geom_point()


```


take the residuals see which is best?




Strictly positive response variable

Box-Cox finds a transformation that makes the transformed response variable close to normally distributed having taken into account the regression model under consideration


## Modeling defective rates

```{r}
boxCox(model)
```
getting the lambda
```{r}
bc<-boxCox(model)

(lambda<-bc$x[which.max(bc$y)])
```

```{r}
summary(model_transform<-lm(sqrt(Defective)~Temperature+Density+Rate, data=dat))
```


## Transforming both the response and predictor variables

when some or all of the predictors and response are highly skewed. Transformations are desirable



Approaches:

1.

transform X_1 X_2 etc.. so that the distribution of the transformed versions are as JOINTLY NORMAL as possible. 

USE:
multivariate version of the box cox



having transformed X_1 X_2 etc.. consider a multivariate regression where you use the transformed variables AND the inverse transformation of the response variable



2.

Transform response and predictor variables simultaneously to joint normaltiy using multivariate genreralization of box cox


EXAMPLE:


understanding relationship between advertising and magazine sales


```{r}
file<-here('data','magazines.csv')
dat<-read_csv(file)
```



```{r}
ggpairs(dat%>%select(c(AdRevenue,AdPages,SubRevenue,NewsRevenue)))
```



all are highly skewed. predictors dont look linearly related. 

consider transformations of the response and predictor variables

```{r}
library(car)
dat<-dat%>%select(c(AdRevenue,AdPages,SubRevenue,NewsRevenue))
powerTransform(dat)
```
all values of lambda are close to zero, so we will use the log transform for all 3 predictors


```{r}
dat_2<-mutate_all(dat,log)
```

```{r}
ggpairs(dat_2)
```


## Using logarithms to estimate percentage effects: Real valued predictor variables


consider

$$log(Y)= \beta_0+\beta_1 log(x_1)+\beta_2 x_2 +e$$

x_2 is a predictor that takes numerical values (x_2 can be a dummy variable)


MATH PROOF

looking at beta_2

slope is change in y/x so log(Y)/x_2=beta_2

delta is b-a so logY2-logY1

logb-loga =log(b/a)

recall: log(1+z) =z (approx)

so we can get rid of the log 

all proving

delY = beta_2 *100 delx_2 (approx)

Thus:

every 1 unit change in x_2, the model predicts a 100*beta % change in y


Example:

newspaper circ

current circ = 210,000

develop model that will predict sunday circ with given weekday circ.


```{r}
file<-here('data','circulation.txt')
dat<-read.table(file, sep="\t", header=TRUE)
```


```{r}
df<-dat%>%select(-Newspaper)
df<-df%>%mutate_at(c("Sunday","Weekday"),log)
ggpairs(df)
```

```{r}
summary(lm.model<-lm(Sunday~.,data=df))
```

Because we used the log transformation on the Y, we can say that the competitor dummy variable represents a 53% decrease in circulation if the newspaper is a tabloid



```{r}
(predictions<-predict(lm.model, newdata=data.frame(Weekday=c(log(210000),log(210000)),competitor=c(0,1)), interval = 'predict'))
```


now convert back to real value

```{r}
as.data.frame(predictions)%>%mutate_all(.,exp)
```

