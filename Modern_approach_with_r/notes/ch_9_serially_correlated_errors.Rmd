---
title: "ch_9_serially_correlated_errors"
author: "Jack Wright"
date: "11/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tidyverse)
library(here)
library(ggResidpanel)
```



in many situations, data is collected over time. 

these often exhibit `serial correlation`

serial correlation:
-resulsts from current time period are correlated with results from earlier time periods
-*violates assumption that the errors are independent*, which is an important assumption for least-squares-based regression 


*autocorrelation*:

-correlation between a variable at different time points. 


*generalized least squares:*
-used to fit models with autocorrelated errors


*transform GLS model to LS model*



## autocorrelation


EX:

estimating price elasticity of a food product

-want to understand effect of price on sales, particularly to develop a model to estimate the percentage effect on sales of a 1% increase in price


consider weekly sales of `Brand 1` at a major US supermarket chain over a year as a function of price each week. 


$$log(Sales_t)=\beta_0+\beta_1log(Price_t)+e$$

subscript t denotes week. (price at week t, sales at week t)


We find a nonrandom pattern in the plot of standardized residuals. 

```{r}
file<-here('data','confood2.txt')
df<-read.table(file, header=TRUE)

lm.mod<-lm(log(Sales)~log(Price), data=df)
           
resid_panel(lm.mod, plots='ls')
```



two other potential predictors:

-week

-promotion: dummy variable, promotion/no promotion

take a look at log(sales) against log(Price)

```{r}
ggplot(df%>%group_by(as.factor(Promotion)), aes(x=log(Price),y=log(Sales)))+geom_point(aes(color=as.factor(Promotion))) +
  theme(legend.position = "top")
```

appears to be linearly related with promotions having a big effect. 

*this analysis ignores that the data was collected over time*



now look at the log(Sales) in week t vs log(sales) in week t-1



```{r}
library(ggpubr)
Sales_t_1<-head(as.vector(df$Sales),-1)
Sales<-df$Sales[2:length(as.vector(df$Sales))]
df.sales<-data.frame(t_1=Sales_t_1,t=Sales)
a<-ggplot(df, aes(x=Week,y=log(Sales)))+geom_point(aes(shape=Promotion, color=Promotion))+geom_line()
b<-ggplot(df.sales, aes(x=log(t_1),y=log(t)))+geom_point()

```

We see that there is a positive correlation between the previous weeks sales (sales_t-1) and the current weeks sales (Sales_t)


Is there also a positive correlation between sales in week t and sales in week t-2, or t-3... 

TOO MUCH WORK. 


use *autocorrelations*

correlation between Y and various values of lagged Y for different periods


**autocorrelation of lag "l" is the correlation between Y and the values of Y lagged by 'l' periods between $Y_t$ and $Y_{t-l}$


$$Autocorrelation(l)=\frac{\sum  (y_t-\bar{y})(y_{t-l}-\bar{y})}{\sum(y_t-\bar{y})^2}$$


```{r}
stats::acf(log(df$Sales),lag.max=17,plot=TRUE)
```


first 17 autocorrelations of log(Sales)

Statistically significantly different if they are outside $\pm 2/n$ where n is number of samples. (the blue lines)

week 1 only exceeds the test, because clearly t-0 is going to have correlation of 1 with t


## Ignoring the autocorrelation effect


DEMONSTRATION OF IGNORANCE OF AUTOCORRELATION

1.

build model without it
(assume errors are independent)

```{r}
lm.ignore<-lm(log(Sales)~log(Price)+Promotion, data=df)
resid_panel(lm.ignore,plots='all')
```


look at the *Index Plot*

highly organized residuals

BOOK SAYS:
postitive (negative) standardized residuals, followed by positive(negative)  standardized residuals, thus there is *positive autocorrelation* present in the residuals. 

To examine futher examine a plot of the autocorrelation function of the standardized residuals from the model

```{r}
a<-acf(rstandard(lm.ignore),lag.max=17, plot=TRUE)
b<-acf(lm.ignore$residuals,lag.max=17, plot=TRUE)


```

my plots arent the same as the book....


## using generalized least squares when the errors are AR(1)

examine methods based on generalized least squares which allow the errors to be autocorrelated (another name is serially correlated).

$$Y_t = \beta_0+\beta_1 x_1+e_t$$

where

$$e_t=\rho e_{t-1}+v_t$$

errors $e_t$ follow an autoregressive process of order 1 (AR(1))

WHAT IS v_t?

it says v_t is iid (every residual is independent and identically distributed) with 
$N(0,\sigma^2)$ (mean zero and variance sigma^2)

the rho e_t-1 term is the signal in the variance from the previous week times a scalar + v_t which is the random variance


The expected value of e_t is rho * the expected value of the time series error + the truly random error = 0
$$E(e_t)=\rho E(e_{t-1})+E(v_t)=0$$


and the variance for e_t is the variance of rho^2variance e times the true varaince ^2


$$\sigma^2$$


since v_t is independent of e_t-1

$$\sigma^2_e=\frac{\sigma^2_v}{1-\rho^2}$$

variance of the full error is equal to the variance of the truly random error/ 1- the constant rho^2


first order autocorrelation among the errors e_t is equal to rho ( i guess the height of the value on the graph is the rho?)

$$Corr(e_t,e_{t-1})=\rho$$



we can also show that the autocorrelation for any level 

$$Corr(e_t,e_{t-l})=\rho^l$$

when rho is less than 1, the correlations get smaller as l increases

MATH looking at the estimate for $\beta_1$

shows how the autocorrelation effects the estimate

*conclusion*

using least squares and ignoring the autocorrelation when it exists will result in consistent estimates of beta_1, but incorrect estimates of the variance of beta_1 least squares. Invalidating the confidence interval and hypothesis test. 


## Generalized least squares estimation

EX:

estimating the price elasticity of a food product continued

```{r}
library(nlme)
gls.food<-gls(log(Sales)~log(Price)+Promotion+Week,data=df, correlation = corAR1(form = ~Week),method='ML')
summary(gls.food)
```

This isn't exactly the output in the book...

the AIC BIC and logLik are all significantly different.

the Coefficients st error t-value and p-value are all pretty close

the residual standard error in mine is a little higher...

the phi is a little higher..

IT IS RIGHT IF WE PUT METHOD='ML'
```{r}
nlme::intervals(gls.food)
```

Output associated with fitting model using maximum liklihood and assuming the errors are AR(1)



```{r}
acf(gls.food$residuals)
```



Looking at the ACF for the residuals (this time they look the same as the book)

we see that the lag 1 autocorrelation at around .6 is highly statistically significant for the GLS residuals


*the high positive autocorrelation that is in the data can still produce nonrandom patterns in the diagnostic plots based on these residuals, even when the fitted model accounts for them and is correct*

We will transform the model with the AR(1) errors into a related model with uncorrelated errors so we can use the diagnostic plots


## transforming a model with AR(1) errors into a model with iid Errors


transform e_t to $\rho e_{t-1}+v_t$

MATH

we get

$$Y_t-\rho Y_{t-1}=(1-\rho)\beta_0+\beta_1(x_t-\rho x_{t-1})+v_t$$



rewrite as

$$Y^*_t=\beta_0 x^*_{t1}+\beta_1x^*_{t2}+v_t~:t=2,....,n$$


this is called the *Cochrane-Orcutt transformation*

since it is only valid for t=2...n we need to deal with the first observation

MATH (not sure if this matters) 


## General approach to transforming GLS to LS

MATH:

recall:

you use thevariance-covariance matrix of the errors multiplied in a certain way with the matrix X (predictors) and Y (dependent variable)

$\sum$ is the variance-covariance matrix, where each element is equal to the covariance of errors between the predictor of that row/column interaction (recall variance within the error of a predictor is along the diagonal)

You can decompose a matrix into upper and lower triangular matrices

so 
$$\sum =S S'$$

where S is the lower triangular matrix with posititve diagonal entries

this is the Cholesky decompostion of $\sum$


it is the 'square root' of the variance-covariance matrix

multiply your model through by the INVERSE of S and look at the error term's variance

$$Var(S^{-1}e=I$$ 
(the identity matrix)

this means *pre multiplying by S (the square root of the variance-covariance matrix of errors) produces a linear model with uncorrelated errors. *

now we can obtain the GLS estimate of the betas using least squares

MATH to prove the betas for the least squares=betas for generalized least squares


EX:

estimating price elasticity of a food product

assume errors are AR(1) using least squares based on the transformed versions of the response and predictor variables


MY TRY:


### how to calculate chocrane-orcutt procedure

1.

store residuals from simple lm in RESI
```{r}
RESI<-lm.ignore$residuals

```

2.

calculate a lag-1 residual variable lagRESI

( i guess this is from a gls with the correlation set to AR(1) and method ='ML')

```{r}
lagRESI<-gls.food$residuals
```

3.

fit a simple linear regression with response RESI and predictor lagRESI and *no intercept*

```{r}
lm.resi<-lm(RESI~lagRESI)
summary(lm.resi)

#get the slope out, this is rho

rho<-lm.resi$coefficients[2]


```
4.

get Y_star and X_star

```{r}
Y_star<-log(df$Sales)-rho*lag(log(df$Sales),1)
X_star<-log(df$Price)-rho*lag(log(df$Price),1)
```



fit simple linear regression with transformed variables

```{r}
summary(lm(Y_star~X_star-1))
```

## Case study

data demonstrates hazards with ignoring autocorrelation in fitting and when examining diagnostics

EX:

saving and loan associations in bay area have monopoly in residential real estate in 1990s


banks had a small portion

Develop a regression model to predict interest rates (Y) from x_1(amount of loands closed in MM) 

x_2 (vacancy index)

```{r}
file<-here('data','BayArea.txt')
bay_area<-read.table(file,header=TRUE)
library(GGally)
ggpairs(bay_area%>%select(c(InterestRate,LoansClosed,VacancyIndex)))
```


FIT A MODEL WITHOUT INCLUDING AUTOCORRELATION

```{r}
summary(lm.bay_ignore<-lm(InterestRate~LoansClosed+VacancyIndex,data=bay_area))
```
```{r}
resid_panel(lm.bay_ignore, plots='all')
```

residuals are highly nonrandom with a quadratic pattern, *obvious autocorrelation* among the standardized residuals

```{r}
acf(rstandard(lm.bay_ignore), lag.max = 12,plot = TRUE,main='Standardized LS residuals')
```



### modeling the autocorrelation effect as AR(1)

THIS IS RIGHT, YOU HAVE TO PUT IN METHOD='ML' for maximum likelihood

```{r}
gls.bay<-gls(data=bay_area, InterestRate~LoansClosed+VacancyIndex,correlation = corAR1(form = ~Month) ,method='ML')
summary(gls.bay)
```
```{r}
nlme::intervals(gls.bay)
```


Use process to assume the errors are AR(1) using least squares based on the transformed versions of predictor and response


### how to calculate chocrane-orcutt procedure

1.

store residuals from simple lm in RESI
```{r}
RESI<-lm.bay_ignore$residuals

```

2.

calculate a lag-1 residual variable lagRESI

( i guess this is from a gls with the correlation set to AR(1) and method ='ML')

```{r}
lagRESI<-gls.bay$residuals
```

3.

fit a simple linear regression with response RESI and predictor lagRESI and *no intercept*

```{r}
lm.resi<-lm(RESI~lagRESI)
summary(lm.resi)

#get the slope out, this is rho

rho<-lm.resi$coefficients[2]




```
4.

get Y_star and X_star

```{r}
Y_star<-bay_area$InterestRate-rho*lag(bay_area$InterestRate,1)
xstarLoansClosed<-bay_area$LoansClosed-lag(rho*bay_area$LoansClosed,1)
xstarVacancyIndex<-bay_area$VacancyIndex-lag(rho*bay_area$VacancyIndex,1)
```



fit simple linear regression with transformed variables

THIS IS WRONG

```{r}
summary(lm_auto<-lm(Y_star~xstarLoansClosed+xstarVacancyIndex))
```

```{r}
df_auto<-data.frame(Y_star,xstarLoansClosed,xstarVacancyIndex)
ggpairs(df_auto)
```

