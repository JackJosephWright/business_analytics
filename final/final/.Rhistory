return(output)
}
df_test<-sample_n(df_raw,10)
df_list<-as.list(df_test$totals)
dict_to_df(df_list)
new_row<-get_dict_items(d[i])
new_row<-get_dict_items(d[1])
rbind.fill(output,new_row)
output=data.frame()
rbind.fill(output,new_row)
bind_rows(output,new_row)
output=new_row
bind_rows(output,new_row)
?bind_rows
bind_rows(list(output,new_row))
bind_rows(list(output,new_row))
bind_cols(output,new_row)
test<-bind_cols(output,new_row)
View(test)
test<-cbind(output,new_row)
test<-rbind.fill(output,new_row)
test<-cbind.fill(output,new_row)
unlink("proposal_cache", recursive = TRUE)
View(df_raw)
View(df_total1)
df <- read.csv("test.csv", header = T, stringsAsFactors = F, colClasses = c(fullVisitorId = "character"))
getwd()
library(here)
df <- read.csv(here('data',"test.csv"), header = T, stringsAsFactors = F, colClasses = c(fullVisitorId = "character"))
df <- read.csv(here('kaggle_files',"test.csv"), header = T, stringsAsFactors = F, colClasses = c(fullVisitorId = "character"))
jsoncars <- toJSON(mtcars, pretty=TRUE)
??toJSON
install.packages('jsonlite')
library(jsonlite)
jsoncars <- toJSON(mtcars, pretty=TRUE)
cat(jsoncars)
fromJSON(jsoncars)
df$device[[1]]
temp<-df$device[1:10]
temp
paste(temp,collapse=",")
a<-paste(temp,collapse=",")
temp
paste("[",a,"]")
b<-paste("[",a,"]")
fromJSON(b,flatten=T)
df_device <- paste("[", paste(df$device, collapse = ","), "]") %>% fromJSON(flatten = T)
library(tidyverse)
df_device <- paste("[", paste(df$device, collapse = ","), "]") %>% fromJSON(flatten = T)
df_geoNetwork <- paste("[", paste(df$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
df_totals <- paste("[", paste(df$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
df_trafficSource <- paste("[", paste(df$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)
write.csv(df, here('kaggle_files','test_clean.csv'), row.names = FALSE)
df_device <- paste("[", paste(df$device, collapse = ","), "]") %>% fromJSON(flatten = T)
df_geoNetwork <- paste("[", paste(df$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
df_totals <- paste("[", paste(df$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
df_trafficSource <- paste("[", paste(df$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)
df <- df %>%
cbind(df_device, df_geoNetwork, df_totals, df_trafficSource) %>%
select(-device, -geoNetwork, -totals, -trafficSource)
factorVars <- c("channelGrouping", "browser", "operatingSystem", "deviceCategory", "country")
df[, factorVars] <- lapply(df[, factorVars], as.factor)
df$transactionRevenue <- as.numeric(df$transactionRevenue)
View(df)
df_small=head(df)
df_raw <- read.csv(here('kaggle_files',"test.csv"), header = T, stringsAsFactors = F, colClasses = c(fullVisitorId = "character"))
df_small=head(df)
df_device <- paste("[", paste(df_small$device, collapse = ","), "]") %>% fromJSON(flatten = T)
df_device <- paste("[", paste(df_small$device, collapse = ","), "]") %>% fromJSON(flatten = T)
df_small
df_small=head(df_raw)
df_device <- paste("[", paste(df_small$device, collapse = ","), "]") %>% fromJSON(flatten = T)
df_device
df_totals <- paste("[", paste(df_small$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
df_totals
df_small$totals
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
file<-here('kaggle_files','train.csv')
df_raw<-read.csv(file)
gc()
library(tidyverse)
library(here)
file<-here('kaggle_files','train.csv')
df_raw<-read.csv(file)
colnames(df)
colnames(df_raw)
df_raw$channelGrouping<-as.factor(df_raw$channelGrouping)
levels(as.factor(df_raw$channelGrouping))
#function for turning python dictionary to a R dataframe
dict_df <-function(d){
}
dict_to_df = function(dict) {
require(plyr)
df = data.frame()
df_temp = list()
store = list()
for (i in 1:length(dict)) {
# Split up the dictionary entry
split = unlist(strsplit(dict[i], '\",'))
split = gsub('\\{', '', split)
split = gsub('\\}', '', split)
values = unlist(strsplit(split, ':'))
# Parse out what will be the df headers
headers = values[seq(1, length(values), 2)]
headers = gsub('\"', '', headers) # Remove quotes
headers = gsub(' ', '', headers)  # and whitespace
# Parse out what will be the df values
row_values = values[seq(0, length(values), 2)]
row_values = gsub('\"', '', row_values) # Remove quotes
row_values = gsub(' ', '', row_values)  # and whitespace
# Construct a dataframe with 1 row
out = data.frame(t(row_values))
colnames(out) = headers
store[i] = list(out)
if (i %% 1000 == 0) { print(round(i / length(dict), 2)) }
}
# rbind all the dataframes together into one dataframe
list_length = length(store)
# If the dictionary is sufficiently large rbind will be slow
# as all hell, so break the rbinding into multiple steps
if (list_length >= 3000) {
no_splits = round(list_length / 500)
chunks = split(store, 1:no_splits)
for (j in 1:no_splits) {
df_temp[j] = list(rbind.fill(chunks[[j]]))
}
df = rbind.fill(df_temp)
return(df)
}
else {
df = rbind.fill(store)
return(df)
}
}
df_total<-dict_to_df(df_raw$totals)
View(df_total)
df<-cbind(df_raw%>%select(-totals),df_total)
df_raw$device[10]
d<-"{\"browser\": \"Firefox\", \"browserVersion\": \"not available in demo dataset\", \"browserSize\": \"not available in demo dataset\", \"operatingSystem\": \"Windows\", \"operatingSystemVersion\": \"not available in demo dataset\", \"isMobile\": false, \"mobileDeviceBranding\": \"not available in demo dataset\", \"mobileDeviceModel\": \"not available in demo dataset\", \"mobileInputSelector\": \"not available in demo dataset\", \"mobileDeviceInfo\": \"not available in demo dataset\", \"mobileDeviceMarketingName\": \"not available in demo dataset\", \"flashVersion\": \"not available in demo dataset\", \"language\": \"not available in demo dataset\", \"screenColors\": \"not available in demo dataset\", \"screenResolution\": \"not available in demo dataset\", \"deviceCategory\": \"desktop\"}"
get_dict_items<-function(d){
#gets dictionary items from a single cell
#remove extraneous characters
dict<-str_remove_all(d,'[\\"\\{\\}]')
#split dict into list items
d_list<-unlist(str_split(dict,","))
#make column header list
headers=str_extract(d_list,'(.+)(?=:)')
#make value list
values<-str_extract(d_list,'(?<=:)(.+)')
output=rbind(values)
colnames(output)<-headers
return(output)
}
dict_to_df<-function(d){
for (i in 1:length(d)){
output=new_row
new_row<-get_dict_items(d[i])
test<-(output,new_row)
d<-"{\"browser\": \"Firefox\", \"browserVersion\": \"not available in demo dataset\", \"browserSize\": \"not available in demo dataset\", \"operatingSystem\": \"Windows\", \"operatingSystemVersion\": \"not available in demo dataset\", \"isMobile\": false, \"mobileDeviceBranding\": \"not available in demo dataset\", \"mobileDeviceModel\": \"not available in demo dataset\", \"mobileInputSelector\": \"not available in demo dataset\", \"mobileDeviceInfo\": \"not available in demo dataset\", \"mobileDeviceMarketingName\": \"not available in demo dataset\", \"flashVersion\": \"not available in demo dataset\", \"language\": \"not available in demo dataset\", \"screenColors\": \"not available in demo dataset\", \"screenResolution\": \"not available in demo dataset\", \"deviceCategory\": \"desktop\"}"
get_dict_items<-function(d){
#gets dictionary items from a single cell
#remove extraneous characters
dict<-str_remove_all(d,'[\\"\\{\\}]')
#split dict into list items
d_list<-unlist(str_split(dict,","))
#make column header list
headers=str_extract(d_list,'(.+)(?=:)')
#make value list
values<-str_extract(d_list,'(?<=:)(.+)')
output=rbind(values)
colnames(output)<-headers
return(output)
}
dict_to_df<-function(d){
for (i in 1:length(d)){
output=new_row
new_row<-get_dict_items(d[i])
test<-rbind(output,new_row)
}
return(output)
}
df_test<-sample_n(df_raw,10)
df_list<-as.list(df_test$totals)
dict_to_df(df_list)
#function for turning python dictionary to a R dataframe
dict_df <-function(d){
}
dict_to_df = function(dict) {
require(plyr)
df = data.frame()
df_temp = list()
store = list()
for (i in 1:length(dict)) {
# Split up the dictionary entry
split = unlist(strsplit(dict[i], '\",'))
split = gsub('\\{', '', split)
split = gsub('\\}', '', split)
values = unlist(strsplit(split, ':'))
# Parse out what will be the df headers
headers = values[seq(1, length(values), 2)]
headers = gsub('\"', '', headers) # Remove quotes
headers = gsub(' ', '', headers)  # and whitespace
# Parse out what will be the df values
row_values = values[seq(0, length(values), 2)]
row_values = gsub('\"', '', row_values) # Remove quotes
row_values = gsub(' ', '', row_values)  # and whitespace
# Construct a dataframe with 1 row
out = data.frame(t(row_values))
colnames(out) = headers
store[i] = list(out)
#if (i %% 1000 == 0) { print(round(i / length(dict), 2)) }
}
# rbind all the dataframes together into one dataframe
list_length = length(store)
# If the dictionary is sufficiently large rbind will be slow
# as all hell, so break the rbinding into multiple steps
if (list_length >= 3000) {
no_splits = round(list_length / 500)
chunks = split(store, 1:no_splits)
for (j in 1:no_splits) {
df_temp[j] = list(rbind.fill(chunks[[j]]))
}
df = rbind.fill(df_temp)
return(df)
}
else {
df = rbind.fill(store)
return(df)
}
}
df_test<-sample_n(df_raw,10)
df_list<-as.list(df_test$totals)
dict_to_df(df_list)
df_device<-dict_to_df(df_raw$device)
#function for turning python dictionary to a R dataframe
dict_df <-function(d){
}
dict_to_df = function(dict) {
require(plyr)
df = data.frame()
df_temp = list()
store = list()
for (i in 1:length(dict)) {
# Split up the dictionary entry
split = unlist(strsplit(dict[i], '\",'))
split = gsub('\\{', '', split)
split = gsub('\\}', '', split)
values = unlist(strsplit(split, ':'))
# Parse out what will be the df headers
headers = values[seq(1, length(values), 2)]
headers = gsub('\"', '', headers) # Remove quotes
headers = gsub(' ', '', headers)  # and whitespace
# Parse out what will be the df values
row_values = values[seq(0, length(values), 2)]
row_values = gsub('\"', '', row_values) # Remove quotes
row_values = gsub(' ', '', row_values)  # and whitespace
# Construct a dataframe with 1 row
out = data.frame(t(row_values))
colnames(out) = headers
store[i] = list(out)
#if (i %% 1000 == 0) { print(round(i / length(dict), 2)) }
}
# rbind all the dataframes together into one dataframe
list_length = length(store)
# If the dictionary is sufficiently large rbind will be slow
# as all hell, so break the rbinding into multiple steps
if (list_length >= 3000) {
no_splits = round(list_length / 500)
chunks = split(store, 1:no_splits)
for (j in 1:no_splits) {
df_temp[j] = list(rbind.fill(chunks[[j]]))
}
df = rbind.fill(df_temp)
return(df)
}
else {
df = rbind.fill(store)
return(df)
}
}
df_total<-dict_to_df(df_raw$totals)
df<-cbind(df_raw%>%select(-totals),df_total)
df_device<-dict_to_df(df_raw$device)
length(df_raw$device)
d<-df_raw$device
strsplit(d[1],'\",')
unlist(strsplit(d[1],'\",'))
split=unlist(strsplit(d[1],'\",'))
gsub('\\{','',split)
split
gsub('\\{','',split)
split
split = gsub('\\{|\\}', '', split)
split
unlist(strsplit(split, ':'))
split = gsub('\\{|\\}', '', split)
#split = gsub('\\}', '', split)
values = unlist(strsplit(split, ':'))
values
# Parse out what will be the df headers
headers = values[seq(1, length(values), 2)]
headers
fromJSON(d)
df_device <- paste("[", paste(df$device, collapse = ","), "]") %>% fromJSON(flatten = T)
View(df_device)
df_device <- paste("[", paste(df_raw$device, collapse = ","), "]") %>% fromJSON(flatten = T)
View(df_device)
df_raw$trafficSource[1]
df_device <- paste("[", paste(df_raw$device, collapse = ","), "]") %>% fromJSON(flatten = T)
df_geoNetwork <- paste("[", paste(df_raw$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
df_totals <- paste("[", paste(df_raw$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
df_trafficSource <- paste("[", paste(df_raw$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)
df <- df_raw %>%
cbind(df_device, df_geoNetwork, df_totals, df_trafficSource) %>%
select(-device, -geoNetwork, -totals, -trafficSource)
factorVars <- c("channelGrouping", "browser", "operatingSystem", "deviceCategory", "country")
df[, factorVars] <- lapply(df[, factorVars], as.factor)
df$transactionRevenue <- as.numeric(df$transactionRevenue)
numVars <- c("visits", "hits", "bounces", "pageviews", "newVisits")
df[, numVars] <- lapply(df[, numVars], as.integer)
df$visitStartTime <- as.POSIXct(df$visitStartTime, tz="UTC", origin='1970-01-01')
write.csv(df, here('data','clean_train_jack.csv'), row.names = FALSE)
write.csv(df, here('kaggle_files','clean_train_jack.csv'), row.names = FALSE)
?write.csv
here('kaggle_files','clean_train_jack.csv')
write.csv(df, './kaggle_files/clean_train_jack.csv', row.names = FALSE)
write.csv(df, 'clean_train_jack.csv', row.names = FALSE)
df <- df %>% mutate(date=ymd(date))
?ymd
library(lubridate)
df <- df %>% mutate(date=ymd(date))
nan_count <- sum(!is.na(df$transactionRevenue))
nan_count
nan_count <- sum(!is.na(df$transactionRevenue))
nan_pct <- nan_count / nrow(df) * 100
nan_pct
total_rev <- sum(df$transactionRevenue, na.rm=T)
total_rev
library(car)
car::powerTransform(df$transactionRevenue)
?gg_miss_var
??gg_miss_var
install.packages('naniar')
library(naniar)
library(naniar)
df <- df %>%
mutate(transactionRevenue=log(transactionRevenue),
transactionRevenue=replace_na(transactionRevenue,0))
gg_miss_var(df %>% filter(transactionRevenue > 0), show_pct = TRUE)
stats::acf(df$transactionRevenue, lag.max=17,plot=TRUE)
nan_count <- sum(!is.na(df$transactionRevenue))
nan_count
df <- df %>%
mutate(transactionRevenue=log(transactionRevenue),
transactionRevenue=replace_na(transactionRevenue,0))
nan_count <- sum(!is.na(df$transactionRevenue))
nan_count
temp<-replace_na(df$transactionRevenue,0)
sum(is.na(temp))
stats::acf(df$transactionRevenue%>%filter(df$transactionRevenue>0), lag.max=17,plot=TRUE)
stats::acf(df$transactionRevenue, lag.max=17,plot=TRUE)
df %>%
select_if(negate(is.numeric))
temp<-df$[1:100]
temp<-df$[1:100,]
temp<-df[1:100,]
view(temp)
stats::acf(df$transactionRevenue, lag.max=3,plot=TRUE)
stats::acf(na.omit(df$transactionRevenue), lag.max=3,plot=TRUE)
stats::acf(na.omit(df$transactionRevenue), lag.max=17,plot=TRUE)
view(df$date)
View(df)
library(lubridate)
df <- df %>% mutate(date=ymd(date))
nan_count <- sum(!is.na(df$transactionRevenue))
nan_pct <- nan_count / nrow(df) * 100
total_rev <- sum(df$transactionRevenue, na.rm=T)
class(df$date)
?stats::acf
df$date[1]
min(df$date)
max(df$date)
View(df)
stats::acf(na.omit(df$transactionRevenue), lag.max=14,plot=TRUE)
df%>%group_by(date)%>%summarize(mean_rev=mean(transactionRevenue))
df %>% group_by(date) %>% summarise(Mean_sales = mean(Sales))
df %>% group_by(date) %>% summarise(Mean_sales = mean(transactionRevenue))
df %>% group_by(as.character(date)) %>% summarise(Mean_sales = mean(transactionRevenue))
df %>% group_by(as.character(date)) %>% summarise(Mean_sales = mean(na.omit(transactionRevenue)))
df %>% group_by_(date) %>% summarise(Mean_sales = mean(na.omit(transactionRevenue)))
df%>%summarise_by_time(.date_var=date,.by='day',value=mean(transactionRevenue))
?summarize_by_time
??summarize_by_time
install.packages('timetk')
library(timetke)
library(timetk)
df%>%summarise_by_time(.date_var=date,.by='day',value=mean(transactionRevenue))
df%>%summarise_by_time(.date_var=date,.by='day',value=mean(na.omit(transactionRevenue)))
timeseries<-df%>%summarise_by_time(.date_var=date,.by='day',value=mean(na.omit(transactionRevenue)))
stats::acf(timeseries,lag.max = 14,plot=TRUE)
stats::acf(na.omit(timeseries$value),lag.max = 14,plot=TRUE)
stats::acf(na.omit(timeseries$value),lag.max = 7,plot=TRUE)
df <- df %>%
df <- df %>%
mutate(transactionRevenue=log(transactionRevenue))
log(1)
gc()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
file<-here('kaggle_files','train.csv')
df_raw<-read.csv(file)
colnames(df_raw)
colnames(df_raw)
df_raw$channelGrouping<-as.factor(df_raw$channelGrouping)
levels(as.factor(df_raw$channelGrouping))
#function for turning python dictionary to a R dataframe
dict_df <-function(d){
}
dict_to_df = function(dict) {
require(plyr)
df = data.frame()
df_temp = list()
store = list()
for (i in 1:length(dict)) {
# Split up the dictionary entry
split = unlist(strsplit(dict[i], '\",'))
split = gsub('\\{|\\}', '', split)
#split = gsub('\\}', '', split)
values = unlist(strsplit(split, ':'))
# Parse out what will be the df headers
headers = values[seq(1, length(values), 2)]
headers = gsub('\"', '', headers) # Remove quotes
headers = gsub(' ', '', headers)  # and whitespace
# Parse out what will be the df values
row_values = values[seq(0, length(values), 2)]
row_values = gsub('\"', '', row_values) # Remove quotes
row_values = gsub(' ', '', row_values)  # and whitespace
# Construct a dataframe with 1 row
out = data.frame(t(row_values))
colnames(out) = headers
store[i] = list(out)
#if (i %% 1000 == 0) { print(round(i / length(dict), 2)) }
}
# rbind all the dataframes together into one dataframe
list_length = length(store)
# If the dictionary is sufficiently large rbind will be slow
# as all hell, so break the rbinding into multiple steps
if (list_length >= 3000) {
no_splits = round(list_length / 500)
chunks = split(store, 1:no_splits)
for (j in 1:no_splits) {
df_temp[j] = list(rbind.fill(chunks[[j]]))
}
df = rbind.fill(df_temp)
return(df)
}
else {
df = rbind.fill(store)
return(df)
}
}
df_total<-dict_to_df(df_raw$totals)
df_device <- paste("[", paste(df_raw$device, collapse = ","), "]") %>% fromJSON(flatten = T)
library(jsonlite)
library(jsonlite)
df_device <- paste("[", paste(df_raw$device, collapse = ","), "]") %>% fromJSON(flatten = T)
df_geoNetwork <- paste("[", paste(df_raw$geoNetwork, collapse = ","), "]") %>% fromJSON(flatten = T)
df_totals <- paste("[", paste(df_raw$totals, collapse = ","), "]") %>% fromJSON(flatten = T)
df_trafficSource <- paste("[", paste(df_raw$trafficSource, collapse = ","), "]") %>% fromJSON(flatten = T)
df <- df_raw %>%
cbind(df_device, df_geoNetwork, df_totals, df_trafficSource) %>%
select(-device, -geoNetwork, -totals, -trafficSource)
factorVars <- c("channelGrouping", "browser", "operatingSystem", "deviceCategory", "country")
df[, factorVars] <- lapply(df[, factorVars], as.factor)
df$transactionRevenue <- as.numeric(df$transactionRevenue)
numVars <- c("visits", "hits", "bounces", "pageviews", "newVisits")
df[, numVars] <- lapply(df[, numVars], as.integer)
df$visitStartTime <- as.POSIXct(df$visitStartTime, tz="UTC", origin='1970-01-01')
write.csv(df, 'clean_train_jack.csv', row.names = FALSE)
getwd()
here()
here::here()
?write.csv
write.csv(df, '/kaggle_files/clean_train_jack.csv', row.names = FALSE)
write.csv(df, './kaggle_files/clean_train_jack.csv', row.names = FALSE)
write.csv(df, '/kaggle_files/clean_train_jack.csv', row.names = FALSE)
write.csv(df, '\kaggle_files\clean_train_jack.csv', row.names = FALSE)
write.csv(df, '.\kaggle_files\clean_train_jack.csv', row.names = FALSE)
write.csv(df, '.\\kaggle_files\\clean_train_jack.csv', row.names = FALSE)
write.csv(df, 'train_clean_jack.csv', row.names = FALSE)
write.csv(df, file='train_clean_jack.csv', row.names = FALSE)
write_csv(df, file='train_clean_jack.csv', row.names = FALSE)
?write_csv
library(readr)
library(readr)
write_csv(df, file='train_clean_jack.csv', row.names = FALSE)
write_csv(df, file='train_clean_jack.csv')
getwd()
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(here)
here::here()
gc()
#load csv
df<-read.csv(here('kaggle_files','train_clean_jack.csv'))
#load csv
df<-read.csv(here('kaggle_files','train_clean_jack.csv'), header=TRUE)
here::here()
getwd()
